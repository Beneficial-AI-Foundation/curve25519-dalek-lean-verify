# Modifications to curve25519-dalek source code

This file contains the diff between the original curve25519-dalek source
and the modified version used in this verification project.

- **Upstream Repository**: https://github.com/dalek-cryptography/curve25519-dalek
- **Upstream Commit**: 8016d6d9b9cdbaa681f24147e0b9377cc8cef934
- **Upstream Tag**: curve25519-4.2.0

---

diff -Naur a/curve25519-dalek/src/backend/serial/curve_models/mod.rs b/curve25519-dalek/src/backend/serial/curve_models/mod.rs
--- a/curve25519-dalek/src/backend/serial/curve_models/mod.rs	2025-07-09 04:41:29.000000000 +0200
+++ b/curve25519-dalek/src/backend/serial/curve_models/mod.rs	2025-10-21 15:16:46.467158069 +0200
@@ -136,6 +136,9 @@
 
 use crate::edwards::EdwardsPoint;
 use crate::field::FieldElement;
+// Only import FieldElement51 when using the serial u64 backend
+#[cfg(all(curve25519_dalek_backend = "serial", curve25519_dalek_bits = "64"))]
+use crate::backend::serial::u64::field::FieldElement51;
 use crate::traits::ValidityCheck;
 
 // ------------------------------------------------------------------------
@@ -383,16 +386,35 @@
         let XX = self.X.square();
         let YY = self.Y.square();
         let ZZ2 = self.Z.square2();
-        let X_plus_Y = &self.X + &self.Y;
-        let X_plus_Y_sq = X_plus_Y.square();
-        let YY_plus_XX = &YY + &XX;
-        let YY_minus_XX = &YY - &XX;
-
-        CompletedPoint {
-            X: &X_plus_Y_sq - &YY_plus_XX,
-            Y: YY_plus_XX,
-            Z: YY_minus_XX,
-            T: &ZZ2 - &YY_minus_XX,
+
+        cfg_if::cfg_if! {
+            if #[cfg(all(curve25519_dalek_backend = "serial", curve25519_dalek_bits = "64"))] {
+                // Use FieldElement51 associated functions directly for serial u64 backend
+                let X_plus_Y = FieldElement51::add(&self.X, &self.Y);
+                let X_plus_Y_sq = X_plus_Y.square();
+                let YY_plus_XX = FieldElement51::add(&YY, &XX);
+                let YY_minus_XX = FieldElement51::sub(&YY, &XX);
+
+                CompletedPoint {
+                    X: FieldElement51::sub(&X_plus_Y_sq, &YY_plus_XX),
+                    Y: YY_plus_XX,
+                    Z: YY_minus_XX,
+                    T: FieldElement51::sub(&ZZ2, &YY_minus_XX),
+                }
+            } else {
+                // Use operators for other backends (fiat or u32)
+                let X_plus_Y = &self.X + &self.Y;
+                let X_plus_Y_sq = X_plus_Y.square();
+                let YY_plus_XX = &YY + &XX;
+                let YY_minus_XX = &YY - &XX;
+
+                CompletedPoint {
+                    X: &X_plus_Y_sq - &YY_plus_XX,
+                    Y: YY_plus_XX,
+                    Z: YY_minus_XX,
+                    T: &ZZ2 - &YY_minus_XX,
+                }
+            }
         }
     }
 }
diff -Naur a/curve25519-dalek/src/backend/serial/scalar_mul/vartime_double_base.rs b/curve25519-dalek/src/backend/serial/scalar_mul/vartime_double_base.rs
--- a/curve25519-dalek/src/backend/serial/scalar_mul/vartime_double_base.rs	2025-07-09 04:41:29.000000000 +0200
+++ b/curve25519-dalek/src/backend/serial/scalar_mul/vartime_double_base.rs	2025-10-20 18:51:03.451744894 +0200
@@ -30,7 +30,9 @@
 
     // Find starting index
     let mut i: usize = 255;
-    for j in (0..256).rev() {
+    let mut j: usize = 256;
+    while j > 0 {
+        j -= 1;
         i = j;
         if a_naf[i] != 0 || b_naf[i] != 0 {
             break;
diff -Naur a/curve25519-dalek/src/backend/serial/u64/field.rs b/curve25519-dalek/src/backend/serial/u64/field.rs
--- a/curve25519-dalek/src/backend/serial/u64/field.rs	2025-07-09 04:41:29.000000000 +0200
+++ b/curve25519-dalek/src/backend/serial/u64/field.rs	2025-10-21 15:16:46.467158069 +0200
@@ -57,47 +57,27 @@
 
 impl<'a> AddAssign<&'a FieldElement51> for FieldElement51 {
     fn add_assign(&mut self, _rhs: &'a FieldElement51) {
-        for i in 0..5 {
-            self.0[i] += _rhs.0[i];
-        }
+        FieldElement51::add_assign(self, _rhs);
     }
 }
 
 impl<'a> Add<&'a FieldElement51> for &FieldElement51 {
     type Output = FieldElement51;
     fn add(self, _rhs: &'a FieldElement51) -> FieldElement51 {
-        let mut output = *self;
-        output += _rhs;
-        output
+        FieldElement51::add(self, _rhs)
     }
 }
 
 impl<'a> SubAssign<&'a FieldElement51> for FieldElement51 {
     fn sub_assign(&mut self, _rhs: &'a FieldElement51) {
-        let result = (self as &FieldElement51) - _rhs;
-        self.0 = result.0;
+        FieldElement51::sub_assign(self, _rhs);
     }
 }
 
 impl<'a> Sub<&'a FieldElement51> for &FieldElement51 {
     type Output = FieldElement51;
     fn sub(self, _rhs: &'a FieldElement51) -> FieldElement51 {
-        // To avoid underflow, first add a multiple of p.
-        // Choose 16*p = p << 4 to be larger than 54-bit _rhs.
-        //
-        // If we could statically track the bitlengths of the limbs
-        // of every FieldElement51, we could choose a multiple of p
-        // just bigger than _rhs and avoid having to do a reduction.
-        //
-        // Since we don't yet have type-level integers to do this, we
-        // have to add an explicit reduction call here.
-        FieldElement51::reduce([
-            (self.0[0] + 36028797018963664u64) - _rhs.0[0],
-            (self.0[1] + 36028797018963952u64) - _rhs.0[1],
-            (self.0[2] + 36028797018963952u64) - _rhs.0[2],
-            (self.0[3] + 36028797018963952u64) - _rhs.0[3],
-            (self.0[4] + 36028797018963952u64) - _rhs.0[4],
-        ])
+        FieldElement51::sub(self, _rhs)
     }
 }
 
@@ -285,6 +265,48 @@
         self.0 = neg.0;
     }
 
+    /// Add two field elements without reduction
+    pub fn add(a: &FieldElement51, b: &FieldElement51) -> FieldElement51 {
+        let mut output = *a;
+        FieldElement51::add_assign(&mut output, b);
+        output
+    }
+
+    /// Add a field element to this one in place
+    pub fn add_assign(a: &mut FieldElement51, b: &FieldElement51) {
+        let mut i = 0;
+        while i < 5 {
+            a.0[i] += b.0[i];
+            i += 1;
+        }
+    }
+
+    /// Subtract two field elements
+    pub fn sub(a: &FieldElement51, b: &FieldElement51) -> FieldElement51 {
+        // To avoid underflow, first add a multiple of p.
+        // Choose 16*p = p << 4 to be larger than 54-bit b.
+        //
+        // If we could statically track the bitlengths of the limbs
+        // of every FieldElement51, we could choose a multiple of p
+        // just bigger than b and avoid having to do a reduction.
+        //
+        // Since we don't yet have type-level integers to do this, we
+        // have to add an explicit reduction call here.
+        FieldElement51::reduce([
+            (a.0[0] + 36028797018963664u64) - b.0[0],
+            (a.0[1] + 36028797018963952u64) - b.0[1],
+            (a.0[2] + 36028797018963952u64) - b.0[2],
+            (a.0[3] + 36028797018963952u64) - b.0[3],
+            (a.0[4] + 36028797018963952u64) - b.0[4],
+        ])
+    }
+
+    /// Subtract a field element from this one in place
+    pub fn sub_assign(a: &mut FieldElement51, b: &FieldElement51) {
+        let result = FieldElement51::sub(a, b);
+        a.0 = result.0;
+    }
+
     /// Given 64-bit input limbs, reduce to enforce the bound 2^(51 + epsilon).
     #[inline(always)]
     fn reduce(mut limbs: [u64; 5]) -> FieldElement51 {
@@ -572,8 +594,10 @@
     /// Returns 2 times the square of this field element.
     pub fn square2(&self) -> FieldElement51 {
         let mut square = self.pow2k(1);
-        for i in 0..5 {
+        let mut i = 0;
+        while i < 5 {
             square.0[i] *= 2;
+            i += 1;
         }
 
         square
diff -Naur a/curve25519-dalek/src/backend/serial/u64/scalar.rs b/curve25519-dalek/src/backend/serial/u64/scalar.rs
--- a/curve25519-dalek/src/backend/serial/u64/scalar.rs	2025-07-09 04:41:29.000000000 +0200
+++ b/curve25519-dalek/src/backend/serial/u64/scalar.rs	2025-10-20 15:12:51.627344330 +0200
@@ -65,10 +65,18 @@
     #[rustfmt::skip] // keep alignment of s[*] calculations
     pub fn from_bytes(bytes: &[u8; 32]) -> Scalar52 {
         let mut words = [0u64; 4];
-        for i in 0..4 {
-            for j in 0..8 {
-                words[i] |= (bytes[(i * 8) + j] as u64) << (j * 8);
-            }
+        let mut i = 0;
+        while i < 4 {
+            let base = i * 8;
+            words[i] = (bytes[base] as u64)
+                | ((bytes[base + 1] as u64) << 8)
+                | ((bytes[base + 2] as u64) << 16)
+                | ((bytes[base + 3] as u64) << 24)
+                | ((bytes[base + 4] as u64) << 32)
+                | ((bytes[base + 5] as u64) << 40)
+                | ((bytes[base + 6] as u64) << 48)
+                | ((bytes[base + 7] as u64) << 56);
+            i += 1;
         }
 
         let mask = (1u64 << 52) - 1;
@@ -88,10 +96,18 @@
     #[rustfmt::skip] // keep alignment of lo[*] and hi[*] calculations
     pub fn from_bytes_wide(bytes: &[u8; 64]) -> Scalar52 {
         let mut words = [0u64; 8];
-        for i in 0..8 {
-            for j in 0..8 {
-                words[i] |= (bytes[(i * 8) + j] as u64) << (j * 8);
-            }
+        let mut i = 0;
+        while i < 8 {
+            // Explicitly unrolled inner loop - 8 steps
+            words[i] |= (bytes[i * 8] as u64) << (0);
+            words[i] |= (bytes[(i * 8) + 1] as u64) << (8);
+            words[i] |= (bytes[(i * 8) + 2] as u64) << (2 * 8);
+            words[i] |= (bytes[(i * 8) + 3] as u64) << (3 * 8);
+            words[i] |= (bytes[(i * 8) + 4] as u64) << (4 * 8);
+            words[i] |= (bytes[(i * 8) + 5] as u64) << (5 * 8);
+            words[i] |= (bytes[(i * 8) + 6] as u64) << (6 * 8);
+            words[i] |= (bytes[(i * 8) + 7] as u64) << (7 * 8);
+            i += 1;
         }
 
         let mask = (1u64 << 52) - 1;
@@ -164,9 +180,11 @@
 
         // a + b
         let mut carry: u64 = 0;
-        for i in 0..5 {
+        let mut i = 0;
+        while i < 5 {
             carry = a[i] + b[i] + (carry >> 52);
             sum[i] = carry & mask;
+            i += 1;
         }
 
         // subtract l if the sum is >= l
@@ -180,27 +198,37 @@
 
         // a - b
         let mut borrow: u64 = 0;
-        for i in 0..5 {
+        let mut i = 0;
+        while i < 5 {
             borrow = a[i].wrapping_sub(b[i] + (borrow >> 63));
             difference[i] = borrow & mask;
+            i += 1;
         }
 
         // conditionally add l if the difference is negative
+        difference.conditional_add_l(Choice::from((borrow >> 63) as u8));
+        difference
+    }
+
+    pub(crate) fn conditional_add_l(&mut self, condition: Choice) -> u64 {
         let mut carry: u64 = 0;
-        for i in 0..5 {
-            let underflow = Choice::from((borrow >> 63) as u8);
-            let addend = u64::conditional_select(&0, &constants::L[i], underflow);
-            carry = (carry >> 52) + difference[i] + addend;
-            difference[i] = carry & mask;
+        let mask = (1u64 << 52) - 1;
+
+        let mut i = 0;
+        while i < 5 {
+            let addend = u64::conditional_select(&0, &constants::L[i], condition);
+            carry = (carry >> 52) + self[i] + addend;
+            self[i] = carry & mask;
+            i += 1;
         }
 
-        difference
+        carry
     }
 
     /// Compute `a * b`
     #[inline(always)]
     #[rustfmt::skip] // keep alignment of z[*] calculations
-    pub (crate) fn mul_internal(a: &Scalar52, b: &Scalar52) -> [u128; 9] {
+    pub fn mul_internal(a: &Scalar52, b: &Scalar52) -> [u128; 9] {
         let mut z = [0u128; 9];
 
         z[0] = m(a[0], b[0]);
@@ -219,7 +247,7 @@
     /// Compute `a^2`
     #[inline(always)]
     #[rustfmt::skip] // keep alignment of return calculations
-    fn square_internal(a: &Scalar52) -> [u128; 9] {
+    pub fn square_internal(a: &Scalar52) -> [u128; 9] {
         let aa = [
             a[0] * 2,
             a[1] * 2,
@@ -316,8 +344,10 @@
     #[inline(never)]
     pub fn from_montgomery(&self) -> Scalar52 {
         let mut limbs = [0u128; 9];
-        for i in 0..5 {
+        let mut i = 0;
+        while i < 5 {
             limbs[i] = self[i] as u128;
+            i += 1;
         }
         Scalar52::montgomery_reduce(&limbs)
     }
diff -Naur a/curve25519-dalek/src/edwards.rs b/curve25519-dalek/src/edwards.rs
--- a/curve25519-dalek/src/edwards.rs	2025-07-09 04:41:29.000000000 +0200
+++ b/curve25519-dalek/src/edwards.rs	2025-10-21 08:39:26.978769720 +0200
@@ -1329,9 +1329,11 @@
         debug_assert!(k > 0);
         let mut r: CompletedPoint;
         let mut s = self.as_projective();
-        for _ in 0..(k - 1) {
+        let mut i = 0;
+        while i < k - 1 {
             r = s.double();
             s = r.as_projective();
+            i += 1;
         }
         // Unroll last iteration so we can go directly as_extended()
         s.double().as_extended()
diff -Naur a/curve25519-dalek/src/scalar.rs b/curve25519-dalek/src/scalar.rs
--- a/curve25519-dalek/src/scalar.rs	2025-07-09 04:41:29.000000000 +0200
+++ b/curve25519-dalek/src/scalar.rs	2025-10-20 16:37:02.522658883 +0200
@@ -1165,8 +1165,10 @@
 
         #[inline]
         fn square_multiply(y: &mut UnpackedScalar, squarings: usize, x: &UnpackedScalar) {
-            for _ in 0..squarings {
+            let mut i = 0;
+            while i <  squarings {
                 *y = y.montgomery_square();
+                i += 1;
             }
             *y = UnpackedScalar::montgomery_mul(y, x);
         }
